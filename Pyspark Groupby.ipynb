{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56ee6941-e228-4d71-a4bb-db5c7524ee07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab8bffdd-3561-4264-9704-9970c9f7382b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\Softwares\\\\Spark\\\\spark-4.0.0-bin-hadoop3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff42a385-6b77-428e-ad3c-9a113e33eff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiate spark\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "c = pyspark.SparkConf().setAppName(\"test_app\").setMaster(\"local\")\n",
    "sc = pyspark.SparkContext(conf = c)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c306636d-8ed1-4092-97c1-b255ea9c9a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ded2540-8f77-4785-bd7b-74d18d19fa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv(\"F:\\\\KSR\\\\KSRDataSets-main\\\\Sample - Superstore.csv\", \\\n",
    "                      header=True, inferSchema=True, escape='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a169ccec-adc5-4522-baf5-5a350ee8194d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.classic.dataframe.DataFrame"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cc372a3-1ec1-4c28-b3be-a68d0ee478ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9994"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1b3ffc7-3e7f-42ef-9e79-10ed681ba4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------+----------+--------------+-----------+-------------+-----------+--------------+-------------+----------+-----------+-------+---------------+----------+------------+-----------------------------------------------------+---------+--------+--------+----------+\n",
      "|Row ID|Order ID      |Order Date|Ship Date |Ship Mode     |Customer ID|Customer Name|Segment    |Country/Region|City         |State     |Postal Code|Region |Product ID     |Category  |Sub-Category|Product Name                                         |Sales    |Quantity|Discount|Profit    |\n",
      "+------+--------------+----------+----------+--------------+-----------+-------------+-----------+--------------+-------------+----------+-----------+-------+---------------+----------+------------+-----------------------------------------------------+---------+--------+--------+----------+\n",
      "|2698  |CA-2016-145317|18-03-2016|23-03-2016|Standard Class|SM-20320   |Sean Miller  |Home Office|United States |Jacksonville |Florida   |32216      |South  |TEC-MA-10002412|Technology|Machines    |Cisco TelePresence System EX90 Videoconferencing Unit|22638.48 |6       |0.5     |-1811.0784|\n",
      "|6827  |CA-2018-118689|02-10-2018|09-10-2018|Standard Class|TC-20980   |Tamara Chand |Corporate  |United States |Lafayette    |Indiana   |47905      |Central|TEC-CO-10004722|Technology|Copiers     |Canon imageCLASS 2200 Advanced Copier                |17499.95 |5       |0.0     |8399.976  |\n",
      "|8154  |CA-2019-140151|23-03-2019|25-03-2019|First Class   |RB-19360   |Raymond Buch |Consumer   |United States |Seattle      |Washington|98115      |West   |TEC-CO-10004722|Technology|Copiers     |Canon imageCLASS 2200 Advanced Copier                |13999.96 |4       |0.0     |6719.9808 |\n",
      "|2624  |CA-2019-127180|22-10-2019|24-10-2019|First Class   |TA-21385   |Tom Ashbrook |Home Office|United States |New York City|New York  |10024      |East   |TEC-CO-10004722|Technology|Copiers     |Canon imageCLASS 2200 Advanced Copier                |11199.968|4       |0.2     |3919.9888 |\n",
      "|4191  |CA-2019-166709|01-01-2018|22-11-2019|Standard Class|HL-15040   |Hunter Lopez |Consumer   |United States |Newark       |Delaware  |19711      |East   |TEC-CO-10004722|Technology|Copiers     |Canon imageCLASS 2200 Advanced Copier                |10499.97 |3       |0.0     |5039.9856 |\n",
      "+------+--------------+----------+----------+--------------+-----------+-------------+-----------+--------------+-------------+----------+-----------+-------+---------------+----------+------------+-----------------------------------------------------+---------+--------+--------+----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "data.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f254c1e-1e1a-449f-a209-d15b7b4ac03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Row ID: integer (nullable = true)\n",
      " |-- Order ID: string (nullable = true)\n",
      " |-- Order Date: string (nullable = true)\n",
      " |-- Ship Date: string (nullable = true)\n",
      " |-- Ship Mode: string (nullable = true)\n",
      " |-- Customer ID: string (nullable = true)\n",
      " |-- Customer Name: string (nullable = true)\n",
      " |-- Segment: string (nullable = true)\n",
      " |-- Country/Region: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Postal Code: integer (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Product ID: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Sub-Category: string (nullable = true)\n",
      " |-- Product Name: string (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- Discount: double (nullable = true)\n",
      " |-- Profit: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a72dab0d-2cad-41b9-b35e-56c659bca235",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data.select(\"Row ID\",\"Segment\",\"Region\",\"Category\",\"Sub-Category\",\"Sales\",\"Quantity\",\"Discount\",\"Profit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ccf4c3b-1006-4267-be87-212fb922e04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+------+---------------+------------+------+--------+--------+------+\n",
      "|Row ID|Segment  |Region|Category       |Sub-Category|Sales |Quantity|Discount|Profit|\n",
      "+------+---------+------+---------------+------------+------+--------+--------+------+\n",
      "|6076  |Corporate|West  |Office Supplies|Storage     |270.62|2       |0.0     |2.7062|\n",
      "+------+---------+------+---------------+------------+------+--------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1.filter(col(\"Row ID\")==6076).show(1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "634d4c1c-36e7-4c65-8e7d-478460668d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+-------+----------+------------+---------+--------+--------+----------+\n",
      "|Row ID|    Segment| Region|  Category|Sub-Category|    Sales|Quantity|Discount|    Profit|\n",
      "+------+-----------+-------+----------+------------+---------+--------+--------+----------+\n",
      "|  2698|Home Office|  South|Technology|    Machines| 22638.48|       6|     0.5|-1811.0784|\n",
      "|  6827|  Corporate|Central|Technology|     Copiers| 17499.95|       5|     0.0|  8399.976|\n",
      "|  8154|   Consumer|   West|Technology|     Copiers| 13999.96|       4|     0.0| 6719.9808|\n",
      "|  2624|Home Office|   East|Technology|     Copiers|11199.968|       4|     0.2| 3919.9888|\n",
      "|  4191|   Consumer|   East|Technology|     Copiers| 10499.97|       3|     0.0| 5039.9856|\n",
      "+------+-----------+-------+----------+------------+---------+--------+--------+----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "data1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e39b84fe-ca5d-47fd-97fd-7bfae605af13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Row ID: integer (nullable = true)\n",
      " |-- Segment: string (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Sub-Category: string (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- Discount: double (nullable = true)\n",
      " |-- Profit: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "118c59e3-d653-48d9-b13c-b7620aa11f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n",
      "|    Segment|       sum(Sales)|\n",
      "+-----------+-----------------+\n",
      "|   Consumer|1161401.344999977|\n",
      "|Home Office|429653.1485000006|\n",
      "|  Corporate|706146.3667999947|\n",
      "+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1 groupby 1 aggregation\n",
    "data1.groupby(\"Segment\").agg(sum(\"Sales\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce8710fe-dd6a-425a-87fa-3651ca402450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+------------------+\n",
      "|    Segment| Region|        sum(Sales)|\n",
      "+-----------+-------+------------------+\n",
      "|Home Office|   West|136721.77699999965|\n",
      "|  Corporate|   West|225855.27449999994|\n",
      "|   Consumer|   West|362880.77300000144|\n",
      "|   Consumer|   East|350908.16700000095|\n",
      "|Home Office|Central| 91212.64399999991|\n",
      "+-----------+-------+------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# 2 groupby 1 aggregation\n",
    "data1.groupby(\"Segment\", \"Region\").agg(sum('Sales')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "924fe159-5eac-4e97-b140-d89eb5d51338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n",
      "| Region|        sum(Sales)|       sum(Profit)|\n",
      "+-------+------------------+------------------+\n",
      "|  South|391721.90500000044| 46749.43030000002|\n",
      "|Central|501239.89080000174|39706.362499999916|\n",
      "|   East| 678781.2399999928| 91522.78000000009|\n",
      "|   West|  725457.824499995|108418.44890000018|\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1 groupby 2 aggregations\n",
    "data1.groupby(\"Region\").agg(sum('Sales'), sum('Profit')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7b4fcb1-6fe5-436d-a71e-1d2e8e67443c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+------------------+------------------+\n",
      "|    Segment| Region|        sum(Sales)|       sum(Profit)|\n",
      "+-----------+-------+------------------+------------------+\n",
      "|Home Office|   West|136721.77699999965|16530.414999999957|\n",
      "|  Corporate|   West|225855.27449999994| 34437.42989999997|\n",
      "|   Consumer|   West|362880.77300000144|  57450.6040000001|\n",
      "|   Consumer|   East|350908.16700000095| 41190.98429999989|\n",
      "|Home Office|Central| 91212.64399999991|12438.412399999997|\n",
      "+-----------+-------+------------------+------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# 2 groupby 2 aggregations\n",
    "data1.groupby('Segment','Region').agg(sum(\"Sales\"), sum('Profit')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dfab64b7-e533-401e-a6a1-fdf9125d265d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+------------------+------------------+\n",
      "|    Segment| Region|        sum(Sales)|       avg(Profit)|\n",
      "+-----------+-------+------------------+------------------+\n",
      "|Home Office|   West|136721.77699999965|28.949938704027947|\n",
      "|  Corporate|   West|225855.27449999994| 35.87232281249997|\n",
      "|   Consumer|   West|362880.77300000144| 34.36040909090915|\n",
      "|   Consumer|   East|350908.16700000095| 28.04015268890394|\n",
      "|Home Office|Central| 91212.64399999991|28.398201826484012|\n",
      "+-----------+-------+------------------+------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# 2 groupby, 2 aggregations --> sum, avg(mean)\n",
    "data1.groupby('Segment','Region').agg(sum(\"Sales\"), avg('Profit')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e77c200-e309-4696-bd48-6998ac0a305a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------------+------------------+\n",
      "| Region|    Segment|       Total_Sales|        Avg_Profit|\n",
      "+-------+-----------+------------------+------------------+\n",
      "|Central|Home Office| 91212.64399999991|28.398201826484012|\n",
      "|   West|Home Office|136721.77699999965|28.949938704027947|\n",
      "|  South|Home Office|        74255.0015| 16.98762610294118|\n",
      "|   East|Home Office| 127463.7259999999|53.205611155378506|\n",
      "|  South|   Consumer|195580.97100000046| 32.11643532219569|\n",
      "+-------+-----------+------------------+------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# aliasing columns\n",
    "data1.groupby('Region','Segment'). \\\n",
    "agg(sum('Sales').alias('Total_Sales'), avg('Profit').alias('Avg_Profit')).show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1bc51255-5785-4051-a8fb-162bbcd2a41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------------------+\n",
      "| Region|  Segment|        sum(Sales)|\n",
      "+-------+---------+------------------+\n",
      "|   West| Consumer|362880.77300000144|\n",
      "|   East| Consumer|350908.16700000095|\n",
      "|Central| Consumer|252031.43400000004|\n",
      "|   West|Corporate|225855.27449999994|\n",
      "|   East|Corporate|200409.34699999998|\n",
      "+-------+---------+------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# 2 groupby, 1 aggregate with sort\n",
    "data1.groupby('Region','Segment').agg(sum('Sales')).sort(desc(\"sum(Sales)\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5bd9e2b-6ad9-40ab-8862-4b07993b59eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------------------+\n",
      "| Region|  Segment|       Total_Sales|\n",
      "+-------+---------+------------------+\n",
      "|   West| Consumer|362880.77300000144|\n",
      "|   East| Consumer|350908.16700000095|\n",
      "|Central| Consumer|252031.43400000004|\n",
      "|   West|Corporate|225855.27449999994|\n",
      "|   East|Corporate|200409.34699999998|\n",
      "+-------+---------+------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# 2 groupby, 2 aggregate with aliasing & sort\n",
    "data1.groupby('Region', 'Segment') \\\n",
    ".agg(sum('Sales').alias('Total_Sales')) \\\n",
    ".sort(desc('Total_Sales')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0a6347d-d7fc-477a-9351-9832a9aadcc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+------------------+------------------+------------------+\n",
      "|    Segment| Region|        sum(Sales)|       sum(Profit)|     avg(Quantity)|\n",
      "+-----------+-------+------------------+------------------+------------------+\n",
      "|Home Office|   West|136721.77699999965|16530.414999999957|3.7810858143607704|\n",
      "|  Corporate|   West|225855.27449999994| 34437.42989999997|           3.78125|\n",
      "|   Consumer|   West|362880.77300000144|  57450.6040000001| 3.873803827751196|\n",
      "|   Consumer|   East|350908.16700000095| 41190.98429999989|3.6398910823689583|\n",
      "|Home Office|Central| 91212.64399999991|12438.412399999997|3.7831050228310503|\n",
      "+-----------+-------+------------------+------------------+------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# 2 groupby, 3 aggregate\n",
    "data1.groupby('Segment','Region').agg(sum('Sales'), sum('Profit'), avg('Quantity')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71ff3513-6904-42d3-bab9-956534906245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+------------------+------------------+------------------+\n",
      "|    Segment|Region|       Total_Sales|      Total_Profit|      Avg_Quantity|\n",
      "+-----------+------+------------------+------------------+------------------+\n",
      "|   Consumer|  West|362880.77300000144|  57450.6040000001| 3.873803827751196|\n",
      "|   Consumer|  East|350908.16700000095| 41190.98429999989|3.6398910823689583|\n",
      "|  Corporate|  West|225855.27449999994| 34437.42989999997|           3.78125|\n",
      "|   Consumer| South|195580.97100000046|26913.572799999987|  3.79236276849642|\n",
      "|Home Office|  East| 127463.7259999999| 26709.21680000001| 3.810756972111554|\n",
      "+-----------+------+------------------+------------------+------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# 2 groupby, 3 aggregate, 2 sort & aliasing  \n",
    "data1.groupby('Segment', 'Region') \\\n",
    "     .agg(sum('Sales').alias('Total_Sales'), \\\n",
    "          sum('Profit').alias('Total_Profit'), \\\n",
    "          avg('Quantity').alias('Avg_Quantity')) \\\n",
    "     .sort(desc('Total_Profit'), desc('Total_Sales')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f2a2873-8382-48ea-ab3e-8e865dc6dfd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+-----------------+------------------+\n",
      "|Sub-Category|       Total_Sales|     Total_Profit|      Avg_Quantity|\n",
      "+------------+------------------+-----------------+------------------+\n",
      "|     Binders|203412.73299999992|30221.76330000009|3.9225213394615888|\n",
      "|   Fasteners| 3024.279999999998|         949.5182| 4.211981566820277|\n",
      "+------------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 groupby, 3 aggregate with Avg_Quantity > 3, 2 sort & aliasing \n",
    "# total Sales & profit, avg quantity for each sub-category & avg quantity > 3.9\n",
    "data1.groupby('Sub-Category') \\\n",
    "     .agg(sum('Sales').alias('Total_Sales'), \\\n",
    "          sum('Profit').alias('Total_Profit'), \\\n",
    "          avg('Quantity').alias('Avg_Quantity')) \\\n",
    "     .filter(col('Avg_Quantity') > 3.9) \\\n",
    "     .sort(asc('Avg_Quantity')) \\\n",
    "     .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec5af601-7cb0-4697-83fa-8286c113c24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+-----------------+------------------+\n",
      "|Sub-Category|       Total_Sales|     Total_Profit|      Avg_Quantity|\n",
      "+------------+------------------+-----------------+------------------+\n",
      "|     Binders|203412.73299999992|30221.76330000009|3.9225213394615888|\n",
      "+------------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1.groupby('Sub-Category') \\\n",
    "     .agg(sum('Sales').alias('Total_Sales'), \\\n",
    "          sum('Profit').alias('Total_Profit'), \\\n",
    "          avg('Quantity').alias('Avg_Quantity')) \\\n",
    "     .filter((col('Avg_Quantity') > 3.9) & (col('Sub-Category') == 'Binders')) \\\n",
    "     .sort(asc('Avg_Quantity')) \\\n",
    "     .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "01a0f34d-1042-4314-aa0d-5c90cbe11332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------------+\n",
      "|  Category|Sub-Category|        sum(Sales)|\n",
      "+----------+------------+------------------+\n",
      "|Technology|     Copiers| 149528.0299999999|\n",
      "|Technology| Accessories|167380.31800000006|\n",
      "|Technology|      Phones| 330007.0539999992|\n",
      "|Technology|    Machines|189238.63100000005|\n",
      "+----------+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  Sub-Category, Category by Technology with total sales\n",
    "data1.groupby('Category', 'Sub-Category') \\\n",
    "     .agg(sum('Sales')) \\\n",
    "     .filter(col('Category') == 'Technology') \\\n",
    "     .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4429ab27-61bd-44a2-8e58-3bcffd049502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------------+-----------------+------------------+\n",
      "|  Category|Sub-Category|       Total_Sales|     Total_Profit|      Avg_Quantity|\n",
      "+----------+------------+------------------+-----------------+------------------+\n",
      "|Technology| Accessories|167380.31800000006|41936.63569999997|              3.84|\n",
      "|Technology|      Phones| 330007.0539999992|44515.73059999997| 3.699662542182227|\n",
      "|Technology|    Machines|189238.63100000005|        3384.7569|3.8260869565217392|\n",
      "+----------+------------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  Sub-Category, Category by Technology with (total sales, total profit, avg quantity)aliasing\n",
    "#  & avg quantity > 3.5\n",
    "data1.groupby('Category', 'Sub-Category') \\\n",
    "     .agg(sum('Sales').alias('Total_Sales'), \\\n",
    "          sum('Profit').alias('Total_Profit'), \\\n",
    "          avg('Quantity').alias('Avg_Quantity')) \\\n",
    "     .filter((col('Category') == 'Technology') & (col('Avg_Quantity') > 3.5)) \\\n",
    "     .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87553155-c191-40e5-ab66-c17f4640ea04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------------+-----------------+------------------+\n",
      "|  Category|Sub-Category|       Total_Sales|     Total_Profit|      Avg_Quantity|\n",
      "+----------+------------+------------------+-----------------+------------------+\n",
      "|Technology|      Phones| 330007.0539999992|44515.73059999997| 3.699662542182227|\n",
      "|Technology|    Machines|189238.63100000005|        3384.7569|3.8260869565217392|\n",
      "|Technology| Accessories|167380.31800000006|41936.63569999997|              3.84|\n",
      "+----------+------------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  Sub-Category, Category by Technology with (total sales, total profit, avg quantity)aliasing\n",
    "#  & avg quantity > 3.5 , sort by Sales in desc \n",
    "data1.groupby('Category', 'Sub-Category') \\\n",
    "     .agg(sum('Sales').alias('Total_Sales'), \\\n",
    "          sum('Profit').alias('Total_Profit'), \\\n",
    "          avg('Quantity').alias('Avg_Quantity')) \\\n",
    "     .filter((col('Category') == 'Technology') & (col('Avg_Quantity') > 3.5)) \\\n",
    "     .sort(desc('Total_Sales')) \\\n",
    "     .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "85395662-ff9b-4eec-b0ac-b3be35a574a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using spark sql \n",
    "data1.createOrReplaceTempView('superstore_tb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "48a0359f-74b6-4656-8bd2-12a51c949c3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Row ID',\n",
       " 'Segment',\n",
       " 'Region',\n",
       " 'Category',\n",
       " 'Sub-Category',\n",
       " 'Sales',\n",
       " 'Quantity',\n",
       " 'Discount',\n",
       " 'Profit']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5beb5905-b25a-432c-a70e-976b6c7d42b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o367.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 56.0 failed 1 times, most recent failure: Lost task 0.0 in stage 56.0 (TID 40) (Shabdaja executor driver): java.io.FileNotFoundException: C:\\Users\\Administrator\\AppData\\Local\\Temp\\blockmgr-d7e3788d-7258-4023-aa32-6b19d5b270f8\\1d\\temp_shuffle_83f40156-b5bb-4dbc-b038-1505f0577701 (The system cannot find the path specified)\r\n\tat java.base/java.io.FileOutputStream.open0(Native Method)\r\n\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:293)\r\n\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:235)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:148)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:168)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:333)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:174)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\r\n\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\nCaused by: java.io.FileNotFoundException: C:\\Users\\Administrator\\AppData\\Local\\Temp\\blockmgr-d7e3788d-7258-4023-aa32-6b19d5b270f8\\1d\\temp_shuffle_83f40156-b5bb-4dbc-b038-1505f0577701 (The system cannot find the path specified)\r\n\tat java.base/java.io.FileOutputStream.open0(Native Method)\r\n\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:293)\r\n\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:235)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:148)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:168)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:333)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:174)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 11\u001b[0m\n\u001b[0;32m      1\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124m    SELECT \u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124m        Category, \u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124m        SUM(Sales) AS Total_Sales, \u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124m        SUM(Profit) AS Total_Profit, \u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124m        AVG(Quantity) AS Avg_Quantity\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124m    FROM superstore_tb\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124m    WHERE Category = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTechnology\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124m    GROUP BY Category\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124m    ORDER BY Total_Sales DESC\u001b[39m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mE:\\Softwares\\Anaconda\\Installation_Files\\Lib\\site-packages\\pyspark\\sql\\classic\\dataframe.py:285\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_show_string(n, truncate, vertical))\n",
      "File \u001b[1;32mE:\\Softwares\\Anaconda\\Installation_Files\\Lib\\site-packages\\pyspark\\sql\\classic\\dataframe.py:303\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    298\u001b[0m         errorClass\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    299\u001b[0m         messageParameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    300\u001b[0m     )\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mshowString(n, \u001b[38;5;241m20\u001b[39m, vertical)\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mE:\\Softwares\\Anaconda\\Installation_Files\\Lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1363\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mE:\\Softwares\\Anaconda\\Installation_Files\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:282\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpy4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    284\u001b[0m     converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mE:\\Softwares\\Anaconda\\Installation_Files\\Lib\\site-packages\\py4j\\protocol.py:327\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    325\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    329\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o367.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 56.0 failed 1 times, most recent failure: Lost task 0.0 in stage 56.0 (TID 40) (Shabdaja executor driver): java.io.FileNotFoundException: C:\\Users\\Administrator\\AppData\\Local\\Temp\\blockmgr-d7e3788d-7258-4023-aa32-6b19d5b270f8\\1d\\temp_shuffle_83f40156-b5bb-4dbc-b038-1505f0577701 (The system cannot find the path specified)\r\n\tat java.base/java.io.FileOutputStream.open0(Native Method)\r\n\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:293)\r\n\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:235)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:148)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:168)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:333)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:174)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\r\n\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\nCaused by: java.io.FileNotFoundException: C:\\Users\\Administrator\\AppData\\Local\\Temp\\blockmgr-d7e3788d-7258-4023-aa32-6b19d5b270f8\\1d\\temp_shuffle_83f40156-b5bb-4dbc-b038-1505f0577701 (The system cannot find the path specified)\r\n\tat java.base/java.io.FileOutputStream.open0(Native Method)\r\n\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:293)\r\n\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:235)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:148)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:168)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:333)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:174)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        Category, \n",
    "        SUM(Sales) AS Total_Sales, \n",
    "        SUM(Profit) AS Total_Profit, \n",
    "        AVG(Quantity) AS Avg_Quantity\n",
    "    FROM superstore_tb\n",
    "    WHERE Category = 'Technology'\n",
    "    GROUP BY Category\n",
    "    ORDER BY Total_Sales DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1029bec3-01d0-49d1-9ced-f5349f923c58",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o372.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 57.0 failed 1 times, most recent failure: Lost task 0.0 in stage 57.0 (TID 41) (Shabdaja executor driver): java.io.FileNotFoundException: C:\\Users\\Administrator\\AppData\\Local\\Temp\\blockmgr-d7e3788d-7258-4023-aa32-6b19d5b270f8\\0d\\temp_shuffle_293ff91c-2f0a-4f55-9664-227dc234dbc9 (The system cannot find the path specified)\r\n\tat java.base/java.io.FileOutputStream.open0(Native Method)\r\n\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:293)\r\n\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:235)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:148)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:168)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:333)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:174)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\r\n\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\nCaused by: java.io.FileNotFoundException: C:\\Users\\Administrator\\AppData\\Local\\Temp\\blockmgr-d7e3788d-7258-4023-aa32-6b19d5b270f8\\0d\\temp_shuffle_293ff91c-2f0a-4f55-9664-227dc234dbc9 (The system cannot find the path specified)\r\n\tat java.base/java.io.FileOutputStream.open0(Native Method)\r\n\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:293)\r\n\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:235)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:148)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:168)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:333)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:174)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 13\u001b[0m\n\u001b[0;32m      1\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124m    SELECT \u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124m        Category, \u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124m        `Sub-Category`, \u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124m        SUM(Sales) AS Total_Sales, \u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124m        SUM(Profit) AS Total_Profit, \u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124m        AVG(Quantity) AS Avg_Quantity\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124m    FROM superstore_tb\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124m    WHERE Category = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTechnology\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124m    GROUP BY Category, `Sub-Category`\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124m    HAVING Avg_Quantity > 3.5\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124m    ORDER BY Total_Sales DESC\u001b[39m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mE:\\Softwares\\Anaconda\\Installation_Files\\Lib\\site-packages\\pyspark\\sql\\classic\\dataframe.py:285\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_show_string(n, truncate, vertical))\n",
      "File \u001b[1;32mE:\\Softwares\\Anaconda\\Installation_Files\\Lib\\site-packages\\pyspark\\sql\\classic\\dataframe.py:303\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    298\u001b[0m         errorClass\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    299\u001b[0m         messageParameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    300\u001b[0m     )\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mshowString(n, \u001b[38;5;241m20\u001b[39m, vertical)\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mE:\\Softwares\\Anaconda\\Installation_Files\\Lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1363\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mE:\\Softwares\\Anaconda\\Installation_Files\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:282\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpy4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    284\u001b[0m     converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mE:\\Softwares\\Anaconda\\Installation_Files\\Lib\\site-packages\\py4j\\protocol.py:327\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    325\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    329\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o372.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 57.0 failed 1 times, most recent failure: Lost task 0.0 in stage 57.0 (TID 41) (Shabdaja executor driver): java.io.FileNotFoundException: C:\\Users\\Administrator\\AppData\\Local\\Temp\\blockmgr-d7e3788d-7258-4023-aa32-6b19d5b270f8\\0d\\temp_shuffle_293ff91c-2f0a-4f55-9664-227dc234dbc9 (The system cannot find the path specified)\r\n\tat java.base/java.io.FileOutputStream.open0(Native Method)\r\n\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:293)\r\n\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:235)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:148)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:168)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:333)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:174)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\r\n\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\nCaused by: java.io.FileNotFoundException: C:\\Users\\Administrator\\AppData\\Local\\Temp\\blockmgr-d7e3788d-7258-4023-aa32-6b19d5b270f8\\0d\\temp_shuffle_293ff91c-2f0a-4f55-9664-227dc234dbc9 (The system cannot find the path specified)\r\n\tat java.base/java.io.FileOutputStream.open0(Native Method)\r\n\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:293)\r\n\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:235)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:148)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:168)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:333)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:174)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        Category, \n",
    "        `Sub-Category`, \n",
    "        SUM(Sales) AS Total_Sales, \n",
    "        SUM(Profit) AS Total_Profit, \n",
    "        AVG(Quantity) AS Avg_Quantity\n",
    "    FROM superstore_tb\n",
    "    WHERE Category = 'Technology'\n",
    "    GROUP BY Category, `Sub-Category`\n",
    "    HAVING Avg_Quantity > 3.5\n",
    "    ORDER BY Total_Sales DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7834cff-498c-4153-a336-61b5f99604fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
